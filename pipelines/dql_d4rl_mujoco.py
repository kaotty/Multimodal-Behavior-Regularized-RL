import os
from copy import deepcopy
import logging
import d4rl
import gym
import hydra
import numpy as np
import torch
import torch.nn.functional as F
import torch.autograd as autograd
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader
import wandb
from cleandiffuser.dataset.d4rl_mujoco_dataset import D4RLMuJoCoTDDataset
from cleandiffuser.dataset.dataset_utils import loop_dataloader
from cleandiffuser.diffusion import DiscreteDiffusionSDE
from cleandiffuser.nn_condition import IdentityCondition
from cleandiffuser.nn_diffusion import DQLMlp
from cleandiffuser.utils import report_parameters, DQLCritic, FreezeModules
from cleandiffuser.STAC.actors.kernels import RBF
from utils import set_seed
import warnings

warnings.filterwarnings("ignore")

def mmd_linear(X, Y):
    """MMD using linear kernel (i.e., k(x,y) = <x,y>)
    Note that this is not the original linear MMD, only the reformulated and faster version.
    The original version is:
        def mmd_linear(X, Y):
            XX = np.dot(X, X.T)
            YY = np.dot(Y, Y.T)
            XY = np.dot(X, Y.T)
            return XX.mean() + YY.mean() - 2 * XY.mean()

    Arguments:
        X {[n_sample1, dim]} -- [X matrix]
        Y {[n_sample2, dim]} -- [Y matrix]

    Returns:
        [scalar] -- [MMD value]
    """
    delta = X.mean(0) - Y.mean(0)
    return delta.dot(delta.T)

def svgd_update(a_0, s, itr_num, svgd_step, batch_size, num_particles, act_dim, alpha, critic, kernel, device):
    """Conduct the SVGD updates of particles.
        
        Inputs:
        - a_0: the initial particles generated by the diffusion model.
          size: [batch_size * num_particles, act_dim]
        - s: the current states (only batch_size different states, each repeated for num_particles times).
          size: [batch_size * num_particles, act_dim]
        - itr_num: the number of iterations of SVGD.
            
        Outputs:
        - a: the particles eventually obtained from SVGD.
          size: [batch_size * num_particles, act_dim]
        - tr: the sum of traces.
          size: [batch_size * num_particles, 1]
        - score: the score function of different actions.
          size: [batch_size, num_particles]
    """
    a = a_0 # [batch_size * num_particles, act_dim]
    tr = torch.zeros(batch_size, num_particles).to(device) 
    for l in range(itr_num):
        q_1, q_2 = critic(s, a)
        q = torch.min(q_1, q_2)
        score_func = autograd.grad(q.sum(), a, retain_graph=True, create_graph=True)[0].to(device) # [batch_size * num_particles, act_dim]
        a, score_func = a.reshape(batch_size, num_particles, act_dim), score_func.reshape(batch_size, num_particles, act_dim)
        # print("a_0:{}".format(a.mean(0).mean(-1)))
        # print(a.size(), score_func.size()) # [100,10,3],[100,10,3]
        K_value, K_diff, K_dist_sq, K_gamma, K_grad = kernel(a, a)
        # print("gamma:{}, dist:{}".format(K_gamma.max(), K_diff.mean()))
        # print(K_value.size(),K_dist_sq.size(),K_gamma.size(),K_grad.size()) # [100,10,10],[100,10,10],[100,1,1],[100,10,10,3]
        h = (K_value.matmul(score_func)/alpha - K_grad.sum(2)) / num_particles
        a, h = a.reshape(-1, act_dim), h.reshape(-1, act_dim) # [batch_size * num_particles, act_dim], [batch_size * num_particles, act_dim]
        q_grad = score_func.abs().sum(-1).mean(-1)
        k_grad = K_grad.sum(2).abs().sum(-1).mean(-1)
        # compute the sum of traces
        term1 = (K_grad * score_func.unsqueeze(1)).sum(-1).sum(-1) / (num_particles-1) / alpha
        term2 = 2 * K_gamma.squeeze(-1) * ((K_grad * K_diff).sum(-1) + K_value * act_dim).sum(-1) / (num_particles-1)
        term3 = (K_grad.permute(0,1,3,2).matmul(score_func.unsqueeze(1))).sum(-1).sum(-1)/(num_particles-1) / alpha
        term4 = (K_value * (2 * act_dim * K_gamma - 4 * K_dist_sq * K_gamma.pow(2))).sum(-1) / (num_particles-1)
        term1, term2 = term1.to(device), term2.to(device)
        # print(term1.mean(), term2.mean(), term3.mean(), term4.mean())
        # print(term1.size(),term2.size()) # [100,10],[100,10]
        tr = tr + svgd_step * (term1 + term2)
        a = a + svgd_step * h
        # print("a:{},h:{}".format(a.reshape(-1,num_particles,act_dim).mean(-1).mean(0), h.reshape(-1,num_particles,act_dim).abs().mean(-1).mean(0)))
        a_svgd = a.reshape(batch_size, num_particles, act_dim)
        a_svgd_mean = a_svgd.mean(1)
        a_svgd_var = (a_svgd - a_svgd_mean.unsqueeze(1)).pow(2).sum(-1).sum(-1) / (num_particles-1)
        a_svgd_cv = a_svgd_var.sqrt() / a_svgd.abs().mean(1).sum(-1)
        a_0 = a_0.reshape(batch_size, num_particles, act_dim)
        a_0_mean = a_0.mean(1)
        a_0_var = (a_0 - a_0_mean.unsqueeze(1)).pow(2).sum(-1).sum(-1) / (num_particles-1)
        a_0_cv = a_0_var.sqrt() / a_0.abs().mean(1).sum(-1)

    score = score_func.sum(-1)
    tr = tr.reshape(batch_size * num_particles, 1)
    return a, tr, score, a_0_cv, a_svgd_cv, q_grad, k_grad


@hydra.main(config_path="../configs/dql/mujoco", config_name="mujoco", version_base=None)
def pipeline(args):

    set_seed(args.seed)

    save_path = f'results/{args.pipeline_name}/{args.task.env_name}/'
    if os.path.exists(save_path) is False:
        os.makedirs(save_path)

    # ---------------------- Create Dataset ----------------------
    env = gym.make(args.task.env_name)
    dataset = D4RLMuJoCoTDDataset(d4rl.qlearning_dataset(env), args.normalize_reward)
    dataloader = DataLoader(
        dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)
    obs_dim, act_dim = dataset.o_dim, dataset.a_dim

    # --------------- Network Architecture -----------------
    nn_diffusion = DQLMlp(obs_dim, act_dim, emb_dim=64, timestep_emb_type="positional").to(args.device)
    nn_condition = IdentityCondition(dropout=0.0).to(args.device)

    wandb.init(project="dql_mujoco", 
               config=dict(args),
               settings=wandb.Settings(init_timeout=120),
               name=f"withtrace-{args.task.env_name}-itr_num:{args.itr_num}-alpha:{args.alpha}-epsilon:{args.svgd_step}-seed:{args.seed}")

    # logger = logging.getLogger('my_logger')
    # logger.setLevel(logging.INFO)
    # script_dir = os.path.dirname(os.path.abspath(__file__))
    # os.makedirs(os.path.join(script_dir,'logs'), exist_ok=True)
    # log_path = os.path.join(script_dir, 'logs', 'latest-{}-itr{}-alpha{}'.format(args.task.env_name,args.itr_num,args.alpha))
    # file_handler = logging.FileHandler(log_path)
    # file_handler.setLevel(logging.INFO)
    # formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    # file_handler.setFormatter(formatter)
    # logger.addHandler(file_handler)

    print(f"======================= Parameter Report of Diffusion Model =======================")
    report_parameters(nn_diffusion)
    print(f"==============================================================================")

    # ---------------------- Actor ----------------------
    actor = DiscreteDiffusionSDE(
        nn_diffusion, nn_condition, predict_noise=args.predict_noise, optim_params={"lr": args.actor_learning_rate},
        x_max=+1. * torch.ones((1, act_dim), device=args.device),
        x_min=-1. * torch.ones((1, act_dim), device=args.device),
        diffusion_steps=args.diffusion_steps, ema_rate=args.ema_rate, device=args.device)
    # ------------------ Critic ---------------------
    critic = DQLCritic(obs_dim, act_dim, hidden_dim=args.hidden_dim).to(args.device)
    critic_target = deepcopy(critic).requires_grad_(False).eval()
    critic_optim = torch.optim.Adam(critic.parameters(), lr=args.critic_learning_rate)

    # ---------------------- Pretraining a diffusion model sampler ----------------------
    actor_lr_scheduler = CosineAnnealingLR(actor.optimizer, T_max=args.gradient_steps)
    critic_lr_scheduler = CosineAnnealingLR(critic_optim, T_max=args.gradient_steps)
    actor.train()
    critic.train()
    pretrain_gradient_step = 0
    log = {"critic_loss": 0., "actor_loss":0., "target_q_mean": 0., "trace": 0.}
    
    if args.mode == 'pretrain': 
        # logger.info("Starting to pretrain the diffusion model.")
        for batch in loop_dataloader(dataloader):

            obs, next_obs = batch["obs"]["state"].to(args.device), batch["next_obs"]["state"].to(args.device)
            act = batch["act"].to(args.device)
            rew = batch["rew"].to(args.device)
            tml = batch["tml"].to(args.device)

            actor_loss = actor.loss(act, obs)
            actor.optimizer.zero_grad()
            actor_loss.backward()
            actor.optimizer.step()
            actor_lr_scheduler.step()
            pretrain_gradient_step += 1
            log["actor_loss"] += actor_loss.item()

            if pretrain_gradient_step % args.log_interval == 0:
                # logger.info("Actor training gradient step:{}, Actor loss:{}".format(pretrain_gradient_step, log["actor_loss"]/args.log_interval))
                log["actor_loss"] = 0

            if pretrain_gradient_step >= args.gradient_steps:
                actor.save(save_path + f'diffusion_ckpt_{args.task}_{pretrain_gradient_step}.pt')
                break

    # ---------------------- Using SVGD to optimize the actions ----------------------
    svgd_gradient_step = 0
    actor.load(save_path + f'diffusion_ckpt_{args.task}_{args.gradient_steps}.pt')
    actor.eval()
    log = {"critic_loss": 0., "target_q_mean": 0., "trace": 0., "score_max": 0., "score_min":0., "score_mean":0., "a_0_cv": 0., "a_svgd_cv": 0., "q_grad": 0., "k_grad": 0.}
    # logger.info("Using SVGD to optimize the actions.")
    
    for batch in loop_dataloader(dataloader): 

        obs, next_obs = batch["obs"]["state"].to(args.device), batch["next_obs"]["state"].to(args.device)
        act = batch["act"].to(args.device)
        rew = batch["rew"].to(args.device)
        tml = batch["tml"].to(args.device)
        prior = torch.zeros((args.batch_size * args.training_num_particles, act_dim), device=args.device)
        kernel = RBF(num_particles=args.training_num_particles, sigma=5, adaptive_sig=0, device=args.device)

        # generate behavioral actions
        next_obs = next_obs.unsqueeze(1).repeat(1, args.training_num_particles, 1).view(-1, obs_dim)
        a_0 = actor.sample(
                prior, solver=args.solver,
                n_samples=args.batch_size * args.training_num_particles, sample_steps=args.sampling_steps, use_ema=False,
                temperature=1.0, condition_cfg=next_obs, w_cfg=1.0, requires_grad=True)[0]
        
        # Critic Training
        current_q1, current_q2 = critic(obs, act)
        next_act, tr, score, a_0_cv, a_svgd_cv, q_grad, k_grad = svgd_update(a_0, next_obs, args.itr_num, args.svgd_step, args.batch_size, args.training_num_particles, act_dim, args.alpha, critic, kernel, args.device)
        # mmd = mmd_linear(a_0, next_act)
        
        target_q_1, target_q_2 = critic_target(next_obs, next_act)
        target_q = torch.min(target_q_1, target_q_2) + args.alpha * tr
        target_q = target_q.view(-1, args.training_num_particles, 1).mean(1)
        target_q = (rew + (1 - tml) * args.discount * target_q).detach()

        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)

        critic_optim.zero_grad()
        critic_loss.backward()
        critic_optim.step()
        critic_lr_scheduler.step()

        # -- ema
        if svgd_gradient_step % args.ema_update_interval == 0:
            # if svgd_gradient_step >= 1000:
            #     actor.ema_update()
            for param, target_param in zip(critic.parameters(), critic_target.parameters()):
                target_param.data.copy_(0.995 * param.data + (1 - 0.995) * target_param.data)

        # ----------- Logging ------------
        log["critic_loss"] += critic_loss.item()
        log["target_q_mean"] += target_q.mean().item()
        log["trace"] += tr.mean().item()
        log["score_max"] += score.max().item()
        log["score_min"] += score.min().item()
        log["score_mean"] += score.mean().item()
        log["a_0_cv"] += a_0_cv.mean().item()
        log["a_svgd_cv"] += a_svgd_cv.mean().item()
        log["q_grad"] += q_grad.mean().item()
        log["k_grad"] += k_grad.mean().item()

        if (svgd_gradient_step + 1) % args.log_interval == 0:
            log["gradient_steps"] = svgd_gradient_step + 1
            log["critic_loss"] /= args.log_interval
            log["target_q_mean"] /= args.log_interval
            log["trace"] /= args.log_interval
            log["score_max"] /= args.log_interval
            log["score_min"] /= args.log_interval
            log["score_mean"] /= args.log_interval
            log["a_0_cv"] /= args.log_interval
            log["a_svgd_cv"] /= args.log_interval
            log["q_grad"] /= args.log_interval
            log["k_grad"] /= args.log_interval
            wandb.log({
                "train/critic_loss": log["critic_loss"],
                "train/target_q_mean": log["target_q_mean"],
                "train/trace": log["trace"],
                "train/score_max": log["score_max"],
                "train/score_min": log["score_min"],
                "train/score_mean": log["score_mean"],
                "train/a_0_cv": log["a_0_cv"],
                "train/a_svgd_cv": log["a_svgd_cv"],
                "train/q_grad": log["q_grad"],
                "train/k_grad": log["k_grad"],
            }, step=svgd_gradient_step)
            # logger.info("Training gradient step:{}, Critic loss:{}, Target q mean:{}, Trace:{}".format(log["gradient_steps"],log["critic_loss"],log["target_q_mean"],log["trace"]))
            # logger.info("Score: max: {}, min:{}, mean:{}".format(log["score_max"],log["score_min"],log["score_mean"]))
            # logger.info("a_0_var: {}, a_svgd_var: {}".format(log["a_0_var"], log["a_svgd_var"]))
            log = {"critic_loss": 0., "target_q_mean": 0., "trace": 0., "score_max": 0., "score_min":0., "score_mean":0., "a_0_cv": 0., "a_svgd_cv": 0., "q_grad": 0., "k_grad": 0.}

        # ----------- Inference ------------
        if (svgd_gradient_step + 1) % args.inference_interval == 0:
            actor.eval()
            critic.eval()
            critic_target.eval()

            env_eval = gym.vector.make(args.task.env_name, args.num_envs)
            normalizer = dataset.get_normalizer()
            episode_rewards = []

            prior = torch.zeros((args.num_envs * args.num_candidates, act_dim), device=args.device)
            for i in range(args.num_episodes):

                obs, ep_reward, cum_done, t = env_eval.reset(), 0., 0., 0

                while not np.all(cum_done) and t < 1000 + 1:
                    # normalize obs
                    obs = torch.tensor(normalizer.normalize(obs), device=args.device, dtype=torch.float32)
                    obs = obs.unsqueeze(1).repeat(1, args.num_candidates, 1).view(-1, obs_dim)
                    act = actor.sample(
                        prior, solver=args.solver,
                        n_samples=args.num_envs * args.num_candidates, sample_steps=args.sampling_steps, use_ema=False,
                        temperature=args.temperature, condition_cfg=obs, w_cfg=1.0, requires_grad=True)[0]

                    kernel = RBF(num_particles=args.num_candidates, sigma=1, adaptive_sig=0, device=args.device)
                    new_act, tr, score, a_0_cv, a_svgd_cv, q_grad, k_grad = svgd_update(act, obs, args.itr_num, args.svgd_step, args.num_envs, args.num_candidates, act_dim, args.alpha, critic, kernel, args.device)

                    # resample
                    with torch.no_grad():
                        q = critic_target.q_min(obs, new_act)
                        q = q.view(-1, args.num_candidates, 1)
                        w = torch.softmax(q * args.task.weight_temperature, 1)
                        new_act = new_act.view(-1, args.num_candidates, act_dim)

                        indices = torch.multinomial(w.squeeze(-1), 1).squeeze(-1)
                        sampled_act = new_act[torch.arange(new_act.shape[0]), indices].cpu().numpy()

                    # step
                    obs, rew, done, info = env_eval.step(sampled_act)

                    t += 1
                    cum_done = done if cum_done is None else np.logical_or(cum_done, done)
                    ep_reward += (rew * (1 - cum_done)) if t < 1000 else rew

                    if np.all(cum_done):
                        break

                episode_rewards.append(ep_reward)

            episode_rewards = [list(map(lambda x: env.get_normalized_score(x), r)) for r in episode_rewards]
            episode_rewards = np.array(episode_rewards)
            wandb.log({
                "inference/reward_mean": np.mean(episode_rewards).item(),
                "inference/reward_std": np.std(episode_rewards).item(),
            }, step=svgd_gradient_step)
            # logger.info("Inference gradient step:{}, mean:{}, std:{}".format(svgd_gradient_step + 1, np.mean(episode_rewards, -1), np.std(episode_rewards, -1)))

            critic.train()

        svgd_gradient_step += 1
        if svgd_gradient_step >= args.gradient_steps:
            torch.save({
                    "critic": critic.state_dict(),
                    "critic_target": critic_target.state_dict(),
                }, save_path + f"critic_ckpt_{args.task}_{svgd_gradient_step}.pt")
            break

if __name__ == "__main__":
    pipeline()
